{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DSKjLklF2Of"
      },
      "source": [
        "# **Falcon Fine-Tuning & Medical RAG Project**\n",
        "### *Developed for FalconForge*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYZ5JFLRGC9T"
      },
      "source": [
        "---\n",
        "\n",
        "In this notebook I will be looking at using a LLM for inference as well as finetuning a LLM for a specific task. We will finish off by working through a RAG example and comparing this to my finetuning approach. \n",
        "\n",
        "Imports for all the code are below. Please run first before anything else!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "rm -rf gtc_llm_session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saOmD1bXFOGd",
        "outputId": "9c29ba3b-c149-411e-ac01-4a88a17e363f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'gtc_llm_session'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 22 (delta 5), reused 0 (delta 0), pack-reused 6 (from 1)\u001b[K\n",
            "Receiving objects: 100% (22/22), 39.15 MiB | 12.88 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes peft datasets einops torch\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from sentence_transformers.util import semantic_search, dot_score\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current folder: /content\n",
            "Files here: ['.config', 'gtc_llm_session', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Current folder:\", os.getcwd())\n",
        "print(\"Files here:\", os.listdir('.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i0wR2N7GkJl"
      },
      "source": [
        "---\n",
        "\n",
        "###Inference with Falcon3-7B-Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqmcBmVxgCPh"
      },
      "source": [
        "We are going to start by using `Falcon3-7B-Base` to perform inference on some prompts we come up with. This will demonstrate how you can use Hugging Face's Transformers library to easily pull down and start using a model. The documentation for the model is [linked here](https://huggingface.co/tiiuae/Falcon3-7B-Base).\n",
        "\n",
        "While using the compute environment provided for GTC we should not have a problem loading a model this size. However, I usually provide this notebook on Google Colab and want to make sure it can be run using their free GPU tier. You may be wondering how I can load and finetune a model with 7 billion parameters using only the T4 GPU provided by the free tier of Google Colab. A T4 GPU has 16GB of GPU RAM which may seem like a lot but to load a 7 billion parameter model at full precision one would have to use at least 28 GB of GPU memory, or almost twice what I have available to us.\n",
        "\n",
        "To make this project happen I will be using the methodology outlined in the paper *QLoRA: Efficient Finetuning of Quantized LLMs* [linked here](https://arxiv.org/abs/2305.14314). It boils down to using reduced precision for my parameters which means I will need significatly less GPU RAM to load the model. In my case I am going to reduce the parameters from 16 bit precision to 4 bit precision, which means I will need appoximately 1/4 the GPU RAM to load my model. This will allow us to use this model for inference with the T4 GPU available to us.\n",
        "\n",
        "Notice the `model` variable in the code cell below. If you would like to try out a different model, replace the contents of this variable with the model you would like to try. Just remember to be mindful of the computational resources you have available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mn3UH2bULwa"
      },
      "source": [
        "The first step is to create my tokenizer so that I can feed my input into the model. Remember that we can't just pass a query into the model, it has to be converted into a numeric format first. To create my tokenizer I am going to use the AutoTokenizer class from HuggingFace. [Visit this link](https://huggingface.co/docs/transformers/v4.32.0/en/model_doc/auto#transformers.AutoTokenizer) to see the documentation for this class. We can pass the name of the model we would like to use to AutoTokenizer and it will automatically set up the tokenizer I need to interact with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235,
          "referenced_widgets": [
            "d37da77ae9b2406f9c63fafade8a2094",
            "aaa6e5aa533a4ded9e71e98a663449f1",
            "a132f068cad74b0b992724080429b233",
            "522fedb2f10e42f9985e59b3fc47d62b",
            "aed80d03cfa945b59ead153ab74d763c",
            "2c969dedfea94717a58ebeca75033821",
            "4b4e8923b5b94b48a3ba2ddfa5b764e7",
            "2ead97a5a664426b9dfd35d2c67dbdfe",
            "79392582c3d94e0c82f90bf111a08d61",
            "b8d4bc02972a42b6b5bfe7529e2e0476",
            "136d69b8452243c7844a6e827ac5d8a7",
            "f54ddd0898b44b8e90e9cc8034b58951",
            "cc726772d0ab4040b131bf99524f80f6",
            "9ac64518551e49c59f3a3f0884a03fdf",
            "61e0065436894c31ac482de893fef781",
            "c20b7a5884ef4331a9b267bde8d6ce90",
            "e0d61f09dc9f4536ba6ccf7ff125c54f",
            "0db6d08fba2a411ea9ce48435f2f50b0",
            "720b37476e49461b8dc450e1875ad5e5",
            "1b8087c399da4fde9562f4cc6cb226e0",
            "6e0d05ba33334cf29c2ed1b7afcbeb1b",
            "a5d797e23bd34a8eaa817fd1a65aa175",
            "c97048d259394fe69204d2fdb58afc9f",
            "87f052fd59c14e3b9b63bf3cba3ff957",
            "c0f74cbb61fc49008e043cd37503d686",
            "7930232dbce9448587a0e9369b7f300d",
            "a0638c510bb040d5a70a81c531866364",
            "4714cb155a9f4365bb3a69f69b1111af",
            "5e1c15c8c4ce4d5a94b09a4ac16b6639",
            "edbb2ec82d3c4cefbbfb7eb39fa3a44d",
            "98465f73e6a6420d97ec010b9b1a3196",
            "d8e8546c7da24f9eacd2d02f7f31f10a",
            "19f63bef99014bc1851bdf7ba3d889f5"
          ]
        },
        "id": "8cDYNKJ-_7tN",
        "outputId": "362094e6-53da-4d22-92f8-d3364fff1d44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f58e8504c45a4aff87aeab6910af8903",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fae7cbe9494342dcabd1e93fa6104b8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "084f657543fa45a2a0c00c34dff291be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0424bdd8fb0b49bdae4bd26f85e537aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/686 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = \"tiiuae/Falcon3-7B-Base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rxZQrJhU7GQ"
      },
      "source": [
        "Next I need to set up my `BitsAndBytesConfig`. We are using the BitsAndBytes library to make my parameter precision changes so that I can load my 7 billion parameter model without running out of GPU RAM in compute limited environments. As you can see below I am loading the model in 4 bit format by using the `load_in_4bit` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ohSvJND8kOtt"
      },
      "outputs": [],
      "source": [
        "bb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqiy0zbOVv4s"
      },
      "source": [
        "Below you can see the code for actually loading my model. We use another Auto class from HuggingFace to do this, this time `AutoModelForCausalLM`. We give it the name of the model we want to load and the BitsAndBytes configuration we just defined. When you run this code cell you should see it load the model which will take a while to download and load into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "19005b60150b48fca075d78d7c128bdc",
            "f60e6bcd4ec84bfc9ed7a0e8e73e8585",
            "d766e83f405d411488228c82563d2cac",
            "49476253993d492f98c4bf0c39ae9f8b",
            "9040ddbf2c0a45f4bf3b57c976f1a1fa",
            "0c7ac335980d4049a03e36ba4e4957f2",
            "700ed08730b84fa09e5e82ae9197ba9a",
            "27a986c7f21441ccbf27dce700007623",
            "1df64be086a047f09413f19290ab6a5f",
            "b5981cea017d4f4dbd96c347fb98f05e",
            "bcb98a84fdc0478faa023f3e943dea85",
            "d7049c2ded754456ae81b2fc188d354a",
            "eb69325e17294056802ba58c5ad0d163",
            "9c3b0af8bd1349a0b57da5fdcfd2c3ed",
            "b6827e84001b42baa56a48072947fdee",
            "6741ac80aa394cb88399dbaa4001dc98",
            "08250aa1cf194c389a81a949a6e07319",
            "eb99134cef5a43aeab15ccc98476cf80",
            "a9359057546f41248d2b83e9bbae1bb4",
            "4e3514134dae4c03ab841f65dc3b2306",
            "5b1de72ffc01436a859546879c6ee62b",
            "7af5c28d309e4c30b2e70cc67e26538d",
            "74def189bded48cba7ac35e98d12545d",
            "17862ba267de4380880ef4749267747c",
            "5b9941b0cc6f401fa90e119224686ca4",
            "596fb4d666724a48a5085a82d7105e42",
            "ae372e00915145398d0843edbcfcd786",
            "78bd0df0742946a39b08ffc029cb98d6",
            "1e29ca41226d482b827d04ae8bf7a5dc",
            "3e0730bdaf244c3496df723ce526481e",
            "74da9391534443519c5b71a83fbcc5fa",
            "9815b5c01ff442ad921d449181e9b5b7",
            "83bda0e020054a89b7a040fe2053e0fe",
            "6f5d7f2dbdff4d59a64a9a4f2051ea20",
            "ed2ccd062cea4e9aa44442c9cc3c6b40",
            "f4b027602dbe47daa30fa467d9a4c9d4",
            "6ac381b8a97a48c6b09db31d18f2f3a9",
            "5fd1d931aeaf4c12be771eb88bccbf57",
            "62e501f0d5eb47a28f363f1d7efa0512",
            "9f8c5b86d07940f99d11987afd1d103c",
            "48d27905750b4c8ba5fd5bb91f07dbc7",
            "f0a790b893b6448cadb98ed89b0564c0",
            "6d49353118964e6ea3661cdbb1c3456a",
            "4639a2ee7ded4522939b657484e30687",
            "eba16f7d697f4e7ca529d74abab5069d",
            "a72eb32811d54d9687e1580341ee7c42",
            "b28a058d1dd440389e12235933f4b5c5",
            "2cb0c998b97a4947a954daff04fa0b87",
            "c33d714f0561495bb3b607f2292ef517",
            "cf4bc53200c14db0a45e9526ebc794e9",
            "9e573b3f69834bc487054d184993cbae",
            "9e8d0ba12f204d2ab3528dcd6c1c624d",
            "ad6d304b47304385bb004d2481a4726e",
            "6b89273fe00b49b6ae797f75c4242455",
            "4a2dd8a26af748fca7c81f573f0ca1f6",
            "1055dafed25b488baf515aaccb2ecfe0",
            "d49bbeb238a14cbca5b7da81daa83bd5",
            "9e54f0081593422d8ccd5ada04a06860",
            "82abdcf98c244b3d9b88c8fef41f2798",
            "d7a61232a19a46db8c3dfc117e915352",
            "d9abe50c17e24ea8bc6446f83780066d",
            "2b4af8e199924402add5e50eae5e0ab8",
            "779e64d02dd74ec8a47cc16518e1bed4",
            "7ae6d2ad51f54880b6398b3a7b15775d",
            "8482c13636f94fa69a95a4d2852fee32",
            "13b4783c67864861969322d7f93d0bc8",
            "6f03adc96336486bad8bac58bee6eb70",
            "3485e1a4bec54acea4799c12a914bd56",
            "a3ae248ae8724453805178bdd4b7385a",
            "3f67b900b4ad4a6091267df5d8ee257a",
            "48a59cf791914fbda4c64a1e744502ca",
            "1eca952ef5fc405d9cf4740090a7e14b",
            "ce246ec878444d1a98f7b8fb91cbdd0f",
            "5d2983ba0c814decbefb8e5ca9c1354c",
            "41681fa00f9f4f7085f3b57c28edc400",
            "32f49fcb5c3e449ca8db828ec88b62d9",
            "c124882328344f268dd87334db68857b",
            "d69bdd3a080e4a4ea08f40263d6d8bd3",
            "6436b334280f45d6a5cb06102ebed056",
            "6978a94c16b7493fac45cb68f2595c7b",
            "0753a25e565a4b13897531d5c3ce5f5c",
            "ae72e7980fb442e789852b4400c8aa2c",
            "668f8ba5f80a4cb5b35b1333fbe92c01",
            "ccbf1f8807c84540984e6eb3c2e6a193",
            "ea6123b286bb45e3a75e6b5f93ce68f5",
            "0585ae130cf34e57aa1db2c0667b9562",
            "e71635e5b9fc4c5b9048592b5f2f2fdc",
            "5acb26910e33406c9b69855e2c332454",
            "fb8535ea3cc443ef9b69b8b1b169694d",
            "e967172958d74b0c9635852988f8f8bd",
            "1e160709ddcf4708a1ba4ebbab9fadda",
            "5cf7a4faf01547c3b1f626685f90e666",
            "25f7d972f8f640ab97c1fd0fe2413bcc",
            "031b9d15f0f943389028eb3dfafea441",
            "d88156e865884235a4e15d7e4ce7bfe4",
            "16f89f5dc0ca41aba4ac1fed52b7440b",
            "329f6157b2cc495db6e48ee8be1a0fb3",
            "0543e472a3724ecd91e385fdc17564e4",
            "c2e1aeb7faf743e5a9408cbdd1f5c302"
          ]
        },
        "id": "ugd7ChTQl7IC",
        "outputId": "77ab003d-a5a2-4657-b385-a3223c868001"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b3c7c671bd347a581fb9f9e27d76548",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fc4b75138ed4b6f93b118206b79a363",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5505dfa9c8e40c0b2658fa002d0f059",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "058c97e67e85478abd2b1097a2b4a5bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/255 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c862d3a68ed94ce9b9ef8d0b996e29ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/113 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "falcon_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model,\n",
        "    quantization_config=bb_config,\n",
        "    use_cache=False,\n",
        "    low_cpu_mem_usage=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djImAPhtiJXn"
      },
      "source": [
        "Now that I have my model loaded let's do some inference examples with it. Below you can see that I have a prompt for my model in the text variable, in this case a question about the national bird of the United States. We need to use my tokenizer to convert my prompt into something I can feed the model. Then I can use the `generate` funtion to give my model my tokenized prompts and save the output as outputs. The printed results say that the national bird is a bald eagle which is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utXYICjRFjBc",
        "outputId": "f33bb4d2-6a63-40f4-d268-3e8750bbaec7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the national bird of the United States? \n",
            " Answer: \n",
            "\n",
            "The national bird of the United States is the Bald Eagle. The Bald Eagle is a symbol of strength, freedom, and the American spirit.\n"
          ]
        }
      ],
      "source": [
        "text = \"Question: What is the national bird of the United States? \\n Answer: \"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "outputs = falcon_model.generate(input_ids=inputs.input_ids, max_new_tokens=30, pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D9Sz-qSkb8j"
      },
      "source": [
        "Below is another similar example with a new prompt. Feel free to change the prompt to experiment with the model. Creating a good prompt is more of an art than a science and you may find that small differences in the way you phrase a question can have large impacts on how the model interprets it and responds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S-8azP_oIlF",
        "outputId": "7fae4032-8a3b-40ce-f733-9ea4cbe0ab9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How do I make teriyaki sauce?\n",
            "OrObject: Teriyaki sauce is a popular Japanese condiment made from soy sauce, mirin, sake, and sugar.\n",
            "\n",
            "Step 1: Gather your ingredients.\n",
            "You will need:\n",
            "- 1/2 cup soy sauce\n",
            "- 1/2 cup mirin (sweet rice wine)\n",
            "- 1/4 cup sake\n",
            "- 1/4 cup granulated sugar\n",
            "- 1 tablespoon grated ginger\n",
            "- 1 clove garlic, minced\n",
            "\n",
            "Step 2: Combine soy sauce, mirin, sake, and sugar in a medium saucepan.\n",
            "Whisk the ingredients together until the sugar is completely dissolved.\n",
            "\n",
            "Step 3: Add grated ginger and minced garlic to the saucepan.\n",
            "Stir well to combine.\n",
            "\n",
            "Step 4: Bring the mixture to a boil over medium heat.\n",
            "Once boiling, reduce the heat to low and simmer for 10 minutes, stirring occasionally.\n",
            "\n",
            "Step 5: Strain the sauce through a fine-mesh sieve into a clean container.\n",
            "This will remove any solids and give you a smooth sauce.\n",
            "\n",
            "Step 6: Store the sauce in the refrigerator for up to 2 weeks.\n",
            "Use it as a marinade or dipping sauce for meats, vegetables, and rice dishes.\n",
            "\n",
            "Step 7: Adjust the seasoning if necessary.\n",
            "If you prefer a sweeter sauce, add more sugar. If you prefer a saltier sauce, add more soy sauce.\n",
            "\n",
            "Step 8: Enjoy your homemade teriyaki sauce!\n",
            "Use it to add flavor to your favorite dishes or experiment with new recipes.\n",
            "\n",
            "Q: How long does it take to make teriyaki sauce?\n",
            "A: It takes about 30 minutes to make teriyaki sauce, including preparation and cooking time.\n",
            "\n",
            "Q: Can I use regular wine instead of mirin?\n",
            "A: Yes, you can use regular wine instead of mirin. However, keep in mind that the flavor profile may be slightly different.\n",
            "\n",
            "Q: Can I make teriyaki sauce ahead of time?\n",
            "A: Yes, you can make teriyaki sauce ahead of time and store it in the refrigerator for up to 2 weeks.\n",
            "\n",
            "Q: Can I freeze teriyaki sauce?\n",
            "A: Yes, you can freeze teriyaki sauce for up to 3 months. Simply transfer the sauce to an airtight container and store it in the freezer.\n",
            "\n",
            "Q: Can I use teriyaki sauce as a\n"
          ]
        }
      ],
      "source": [
        "text2 = \"How do I make teriyaki sauce?\"\n",
        "\n",
        "inputs = tokenizer(text2, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "outputs = falcon_model.generate(input_ids=inputs.input_ids, max_new_tokens=500, pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buP7mmWrpOuO"
      },
      "source": [
        "Now let's change things up a bit. For the finetuning portion of this project I am going to be using the MedText dataset. This dataset contains blurbs of patient symptoms and then an associated diagnosis and action plan to treat the symptoms. The hope with my finetuning is that I can give the model a description of symptoms as the prompt and then produce a reasonable response as output. Let's test my current model to see how it performs on this task as a baseline. Below you can see one of the entries in the MedText dataset as my prompt.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klSkaUfR0liX",
        "outputId": "7f513928-dc6c-4765-bcdc-3e2298d38ae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A 25-year-old female presents with swelling, pain, and inability to bear weight on her left ankle following a fall during a basketball game where she landed awkwardly on her foot. The pain is on the outer side of her ankle. What is the likely diagnosis and next steps? (Choose all that apply)\n",
            "\n",
            "A. Ankle sprain\n",
            "B. Lateral ankle ligament tear\n",
            "C. Medial malleolar fracture\n",
            "D. Peroneal tendon injury\n",
            "E. Achilles tendon rupture\n",
            "\n",
            "**Answer:**\n",
            "A. Ankle sprain\n",
            "B. Lateral ankle ligament tear\n",
            "D. Peroneal tendon injury\n",
            "\n",
            "**Explanation:**\n",
            "- **Ankle sprain:** The patient's symptoms of swelling, pain, and inability to bear weight on the ankle are consistent with\n"
          ]
        }
      ],
      "source": [
        "text3 = \"A 25-year-old female presents with swelling, pain, and inability to bear weight on her left ankle following a fall during a basketball game where she landed awkwardly on her foot. The pain is on the outer side of her ankle. What is the likely diagnosis and next steps? \"\n",
        "\n",
        "inputs = tokenizer(text3, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "outputs = falcon_model.generate(input_ids=inputs.input_ids, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH3EQMVOZgm5"
      },
      "source": [
        "We can see that the response from my model is reasonable but not in the format or detail that I am hoping for. It tried to format a response as a multiple choice question which is not what we want in this case. Keep this response in mind as I will compare it to the output generated by my finetuned model at the end of this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K2wsULqGVGB"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "###Finetuning on the MedText Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP1diI2MZ20W"
      },
      "source": [
        "As promised I am going to finetune my Falcon model using the MedText dataset. The first step is going to be to define some arguments for training. Below you can see the configuration I am going to be using for this which is saved as `training_args`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4axtB_ZLGllx"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetuned_falcon\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16 = True,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    logging_steps=1,\n",
        "    num_train_epochs=1,\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz8_QUH4aIKD"
      },
      "source": [
        "We are going to be using the `peft` library to make finetuning possible on the hardware I have in this notebook. What this will allow us to do is finetune on some \"adapter\" parameters that will be incorperated into my existing model rather than attempting to finetune all 7 billion parameters we already have. This will make training much more feasible computationally and also keep the model from \"forgetting\" things it already knows in the finetuning process. Below you can see the configuration I am going to use to make this happen. To determine what to put in `target_modules` it can be helpful to print out the model and find all the `Linear4bit` layers. We use that configuration along with the `get_peft_model` method to generate a new model that has adapters I can finetune, which we store into the variable `lora_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8LGkGxApDXl",
        "outputId": "24bc689b-f2f1-4589-8c5a-38f6da4c1b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(131072, 3072)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=3072, out_features=23040, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=3072, out_features=23040, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=23040, out_features=3072, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-06)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((3072,), eps=1e-06)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=131072, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(falcon_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CD-O9NTrG_2A"
      },
      "outputs": [],
      "source": [
        "falcon_model.gradient_checkpointing_enable()\n",
        "falcon_model = prepare_model_for_kbit_training(falcon_model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(falcon_model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1HOjwhXc3rj"
      },
      "source": [
        "The function below counts and prints out  the number and percentage of parameters I will be using to train compared to the total number of parameters in the model. You should see that I will be training only ~0.12% of the total parameters which tells us my LoRa setup was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrOwpfU-N0TD",
        "outputId": "75d54602-8f15-4641-bc50-9b10e2f94a55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 11,067,392 || all params: 7,466,617,856 || trainable%: 0.1482\n"
          ]
        }
      ],
      "source": [
        "lora_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t71kKOcsbPmK"
      },
      "source": [
        "Now I need my dataset that I am going to use for finetuning. We are going to use MedText which is hosted on the HuggingFace Hub. This means I can use the `load_dataset` function to import my dataset as seen below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "1c66d38c456d4ebd9aae80058107a618",
            "eb7364c9986245a08c7ecb2529a74b9b",
            "e0fccdb985824247ad72d787d3d773ff",
            "a4fbf948c53649cbbf18b5846677987f",
            "ed61d0b7085348509fa65d6a29a9e8d3",
            "0c6f800bae6644fe9ea46e18565cad53",
            "efd4ce0e88b74e4ab7007b1e031bdd85",
            "93d3f5dd56664fe68787f0744652dfd6",
            "d666f0b84e024e91b2e26b5f2568f387",
            "e92502bb2abc435e94778df764edc120",
            "8f7cf79a20244fe7b2e890bd42fb2db7",
            "8b9c4334a7844096996fa44dbb29324a",
            "9eba13af351643c6a417c8498d42b67d",
            "d5062d8ffb0749c3b993350046267b49",
            "ee60eaf50a344f5888fa958c140b34a8",
            "438ac711c35a406cbb0875fd31fbff27",
            "73cc90686edc42b9a53aae3f173dfeb4",
            "72a86ccdce43410ba5bc8b2e70bf7ac3",
            "7ff3ed40eeec475d951bb2b6bae37ddb",
            "5ddba2b01e4b4f28aa0ea429617b2674",
            "f9a40f8c21be40c087d779f3c9b9707f",
            "c2fbaec077444e74bf9947f2cb87f9de",
            "06f146e4690b4cfbb22e1c71f27b3a7b",
            "d1dad1f4b55b48158700ed7c4641a0f2",
            "4e4a32f42e8f403cbce6c963adf446a5",
            "2fc0425ddb48467f9456f4c3335b52d6",
            "63d666a013b3434e840ed3e9c14d22a8",
            "a0530658209440439e4c8c8d7b83be84",
            "d79e518cf2cb4d82b1d432a10cd90105",
            "1c24f773977347e58be866d1729094bd",
            "ddb18bdd25954061839c705857d3dba3",
            "a6e355ba08ec4c9a9b0c1b3761c4995e",
            "9c2a1801e1674a65a7555beb0799bc92"
          ]
        },
        "id": "H__af67wPrEm",
        "outputId": "aaa1ccac-b77b-4ca0-8cee-502c159c2f19"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85ade682f25b4b97832194e2ab20ca41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "661acbd14bfd4193ae8351a944af8555",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "medtext_2.csv: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54b163d0d0a34578a0e6d287739fdf29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/1412 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset(\"BI55/MedText\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT-uUlbKbhG2"
      },
      "source": [
        "For finetuning I am going to combine my prompts and responses. Below you can see where we create a Pandas DataFrame from my dataset, grab the Prompt and Completion columns, and combine both in a new Info column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IG1YePHNbfRI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "prompt = df.pop(\"Prompt\")\n",
        "comp = df.pop(\"Completion\")\n",
        "df[\"Info\"] = prompt + \"\\n\" + comp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSyj7631yj4X",
        "outputId": "90bd0908-3b41-4d2d-a145-0e0c32e94b1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A 50-year-old male presents with a history of recurrent kidney stones and osteopenia. He has been taking high-dose vitamin D supplements due to a previous diagnosis of vitamin D deficiency. Laboratory results reveal hypercalcemia and hypercalciuria. What is the likely diagnosis, and what is the treatment?\n",
            "This patient's history of recurrent kidney stones, osteopenia, and high-dose vitamin D supplementation, along with laboratory findings of hypercalcemia and hypercalciuria, suggest the possibility of vitamin D toxicity. Excessive intake of vitamin D can cause increased absorption of calcium from the gut, leading to hypercalcemia and hypercalciuria, which can result in kidney stones and bone loss. Treatment would involve stopping the vitamin D supplementation and potentially providing intravenous fluids and loop diuretics to promote the excretion of calcium.\n"
          ]
        }
      ],
      "source": [
        "print(df[\"Info\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LlbYayDb76J"
      },
      "source": [
        "A lot of the work that goes into finetuning is preprocessing the data you wish to feed the model. Not only do I need to run all my data through my tokenizer, but we also need to ensure that we keep the dimensions of my data in line with what the model expects. Below you can see the function I used to handle the tokenization and returning the tokenized results. This function is based on the tokenizing function written by Harveen Singh Chanda which is linked in the comment below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CZymclBNfCoR"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/harveenchadha/tokenize-train-data-using-bert-tokenizer\n",
        "def tokenizing(text, tokenizer, chunk_size, maxlen):\n",
        "    input_ids = []\n",
        "    tt_ids = []\n",
        "    at_ids = []\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        text_chunk = text[i:i+chunk_size]\n",
        "        encs = tokenizer(\n",
        "                    text_chunk,\n",
        "                    max_length = 2048,\n",
        "                    padding='max_length',\n",
        "                    truncation=True,\n",
        "                    )\n",
        "\n",
        "        input_ids.extend(encs['input_ids'])\n",
        "\n",
        "    return {'input_ids': input_ids}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rfEtj7AcdGH"
      },
      "source": [
        "Now I can use this function to preprocess my data. We pass the `Info` column we created earlier into this function as a list along with the tokenizer we intilalized earlier and receive tokens as output. We can use the HuggingFace Datasets object to easily create a dataset from these tokens and then split the dataset into `train` and `test` subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmFu1M_rVZVz",
        "outputId": "cad00bb5-3eff-4214-b6ce-2f0b3f20e981"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 1129\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 283\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = tokenizing(list(df[\"Info\"]), tokenizer, 256, 2048)\n",
        "tokens_dataset = Dataset.from_dict(tokens)\n",
        "split_dataset = tokens_dataset.train_test_split(test_size=0.2)\n",
        "split_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f387b969",
        "outputId": "94bd2e3d-7e73-4cc6-f77c-de13b9ff80d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access './finetuned_falcon/checkpoint-1000': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls ./finetuned_falcon/checkpoint-1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d96d0016",
        "outputId": "ca213e2f-0cc9-4d21-eca9-d00d3b7d39fd"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './finetuned_falcon/checkpoint-1000/adapter_config.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3342397431.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./finetuned_falcon/checkpoint-1000/adapter_config.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mconfig_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './finetuned_falcon/checkpoint-1000/adapter_config.json'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open('./finetuned_falcon/checkpoint-1000/adapter_config.json', 'r') as f:\n",
        "    config_data = json.load(f)\n",
        "\n",
        "print(json.dumps(config_data, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00f62185",
        "outputId": "e4ae3e44-e478-4eb9-9829-c23167f0f580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access './finetuned_falcon': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls ./finetuned_falcon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThDnA3BIc3o0"
      },
      "source": [
        "We now only need one more piece before I can start training - the Trainer itself. Below you can see that we initialize the `trainer` by giving it the LoRA model we created, my training arguments configuration, and my dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vXc7gozgO-89"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=split_dataset[\"train\"],\n",
        "    eval_dataset=split_dataset[\"test\"],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyqPXvfWdTwT"
      },
      "source": [
        "I will now perform the finetuning. This process will take some time depending on your hardware.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "3UeZAYhqPAi6",
        "outputId": "0721e167-cffd-4396-f3c3-3eba0abb81b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='69' max='1129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  69/1129 10:18 < 2:42:59, 0.11 it/s, Epoch 0.06/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()\n",
        "trainer.model.save_pretrained(\"./finetuned_falcon\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OPp_yTLdnTS"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "###Testing the Finetuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU0tEcXbdyae"
      },
      "source": [
        "Now that my finetuning is complete it's time to see if it gives a better response to the MedText data than it did before. We first need to load in my finetuned model as seen below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a85dy5F53PuG",
        "outputId": "cae2291a-d9c5-4387-eb0a-501ce7ed9ffc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "\n",
        "config = PeftConfig.from_pretrained('./gtc_llm_session')\n",
        "\n",
        "# if using already finetuned files\n",
        "finetuned_model = PeftModel.from_pretrained(falcon_model, './gtc_llm_session')\n",
        "\n",
        "# comment out line 6 and uncomment this if you are finetuning in this notebook\n",
        "#finetuned_model = PeftModel.from_pretrained(falcon_model, './finetuned_falcon')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpB0rt5dd9NV"
      },
      "source": [
        "Then I will do inference just like we did before with the same prompt used previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DIquQ2x34BF",
        "outputId": "0bf3bf4a-7f1c-4657-8b11-6898372f7359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A 25-year-old female presents with swelling, pain, and inability to bear weight on her left ankle following a fall during a basketball game where she landed awkwardly on her foot. The pain is on the outer side of her ankle. What is the likely diagnosis and next steps?\n",
            "This patient's symptoms are suggestive of an ankle sprain, specifically an inversion sprain, which is common in basketball players. The next steps would include a physical examination to assess the severity of the sprain, including range of motion, swelling, and tenderness. An X-ray may be ordered to rule out any fractures. Treatment typically involves rest, ice, compression, and elevation\n"
          ]
        }
      ],
      "source": [
        "text4 = \"A 25-year-old female presents with swelling, pain, and inability to bear weight on her left ankle following a fall during a basketball game where she landed awkwardly on her foot. The pain is on the outer side of her ankle. What is the likely diagnosis and next steps?\"\n",
        "\n",
        "inputs = tokenizer(text4, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "outputs = finetuned_model.generate(input_ids=inputs.input_ids, max_new_tokens=75, pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c-GugmUeCG8"
      },
      "source": [
        "As you can see the response is now much more similar to the MedText responses. Not only are the recommendations the model gives reasonable but the way the responses are written are also much more cohesive and professional than the multiple choice question format it gave us before. This shows how finetuning can be used to change the behavior of a LLM pretty dramatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh1oqYYs9iBb"
      },
      "source": [
        "#**RAG Example**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX_b5xrD_EAv"
      },
      "source": [
        "In this section I will be going through a basic RAG (Retrieval-Augmented Generation) workflow using a dataset of recipes and [Falcon3-7B-Base](https://huggingface.co/tiiuae/Falcon3-7B-Base). The goal will be to see the model use specific information from [this recipe dataset](https://huggingface.co/datasets/Shengtao/recipe) in its responses after we build out the RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMsYWp2H_i_4"
      },
      "source": [
        "---\n",
        "### Baseline Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcC_F7N8_lpw"
      },
      "source": [
        "Let's set a baseline of what kind of responses my model gives us without using RAG so that I can hopefully see a difference with RAG. First we set up the prompts we would like to use. Feel free to edit or add to these if you are curious about another prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lPXKjqmG_oNx"
      },
      "outputs": [],
      "source": [
        "prompt_task = \"Question: \"\n",
        "prompt_end = \"\\nAnswer: \"\n",
        "\n",
        "prompts = [\n",
        "    \"What two ingredients do I need for two-ingredient pizza dough?\",\n",
        "    \"What do I need for a quick tartar sauce?\",\n",
        "    \"What should I preheat the oven to to make blueberry muffins?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5WiPsnL_vQ5"
      },
      "source": [
        "Now we feed them into my model to see what responses we get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnqEVQ2c_w0x",
        "outputId": "cbaa707f-52d5-47c2-9125-5a4fccd5ce5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Question: What two ingredients do I need for two-ingredient pizza dough?\n",
            "Answer: 2 cups of flour and 1 cup of water.\n",
            "\n",
            "Question: What is the main ingredient in a pizza?\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What do I need for a quick tartar sauce?\n",
            "Answer: 1/2 cup mayonnaise, 1 tablespoon lemon juice, 1 tablespoon sweet pickle relish, 1/2 teaspoon salt\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What should I preheat the oven to to make blueberry muffins?\n",
            "Answer: 350 degrees Fahrenheit.\n",
            "\n",
            "Question: What is the capital of France?\n",
            "Answer: Paris.\n",
            "\n",
            "Question\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "All done\n"
          ]
        }
      ],
      "source": [
        "for prompt in prompts:\n",
        "  prompt = prompt_task + prompt + prompt_end\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "  generate_ids = falcon_model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=25, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(tokenizer.decode(generate_ids[0]))\n",
        "  print(\"\\n------------------------------------------------------------\")\n",
        "\n",
        "print(\"\\n\\nAll done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk2n-_pj_1uH"
      },
      "source": [
        "---\n",
        "### Import Recipe Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BFQwcCs_4jj"
      },
      "source": [
        "The results seem reasonable. However we would like my model to use recipe information from a recipe dataset rather than information it remembers from its pretraining. To start that process we first need to import my dataset. We download it from Hugging Face and grab only the first 500 entries. This is only done to save time for this demo because creating the embeddings for all entries in the dataset takes a lot of time and, since this is just a demo, we don't need all of the dataset entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BnPr1xD3_3b2",
        "outputId": "0a80e626-f728-46ad-f55b-a19a87da9b5b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b785fe45-ac1d-4d15-8b24-84d10a77e99d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>category</th>\n",
              "      <th>author</th>\n",
              "      <th>description</th>\n",
              "      <th>rating</th>\n",
              "      <th>rating_count</th>\n",
              "      <th>review_count</th>\n",
              "      <th>ingredients</th>\n",
              "      <th>directions</th>\n",
              "      <th>...</th>\n",
              "      <th>vitamin_k_mcg</th>\n",
              "      <th>biotin_mcg</th>\n",
              "      <th>vitamin_b12_mcg</th>\n",
              "      <th>mono_fat_g</th>\n",
              "      <th>poly_fat_g</th>\n",
              "      <th>trans_fatty_acid_g</th>\n",
              "      <th>omega_3_fatty_acid_g</th>\n",
              "      <th>omega_6_fatty_acid_g</th>\n",
              "      <th>instructions_list</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Simple Macaroni and Cheese</td>\n",
              "      <td>https://www.allrecipes.com/recipe/238691/simpl...</td>\n",
              "      <td>main-dish</td>\n",
              "      <td>g0dluvsugly</td>\n",
              "      <td>A very quick and easy fix to a tasty side-dish...</td>\n",
              "      <td>4.42</td>\n",
              "      <td>834</td>\n",
              "      <td>575</td>\n",
              "      <td>1 (8 ounce) box elbow macaroni ; \u00bc cup butter ...</td>\n",
              "      <td>Bring a large pot of lightly salted water to a...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['Bring a large pot of lightly salted water to...</td>\n",
              "      <td>https://www.allrecipes.com/thmb/GZrTl8DBwmRuor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Gourmet Mushroom Risotto</td>\n",
              "      <td>https://www.allrecipes.com/recipe/85389/gourme...</td>\n",
              "      <td>main-dish</td>\n",
              "      <td>Myleen Sagrado Sj\u00f6din</td>\n",
              "      <td>Authentic Italian-style risotto cooked the slo...</td>\n",
              "      <td>4.80</td>\n",
              "      <td>3388</td>\n",
              "      <td>2245</td>\n",
              "      <td>6 cups chicken broth, divided ; 3 tablespoons ...</td>\n",
              "      <td>In a saucepan, warm the broth over low heat. W...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['Warm broth in a saucepan over low heat.', 'M...</td>\n",
              "      <td>https://www.allrecipes.com/thmb/xCk4IEjfAYBikO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dessert Crepes</td>\n",
              "      <td>https://www.allrecipes.com/recipe/19037/desser...</td>\n",
              "      <td>breakfast-and-brunch</td>\n",
              "      <td>ANN57</td>\n",
              "      <td>Essential crepe recipe.  Sprinkle warm crepes ...</td>\n",
              "      <td>4.80</td>\n",
              "      <td>1156</td>\n",
              "      <td>794</td>\n",
              "      <td>4  eggs, lightly beaten ; 1\u2009\u2153 cups milk ; 2 ta...</td>\n",
              "      <td>In large bowl, whisk together eggs, milk, melt...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['Whisk together eggs, milk, flour, melted but...</td>\n",
              "      <td>https://www.allrecipes.com/thmb/VwULr05JFDluPI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pork Steaks</td>\n",
              "      <td>https://www.allrecipes.com/recipe/70463/pork-s...</td>\n",
              "      <td>meat-and-poultry</td>\n",
              "      <td>BABYLOVE1222</td>\n",
              "      <td>My mom came up with this recipe when I was a c...</td>\n",
              "      <td>4.57</td>\n",
              "      <td>689</td>\n",
              "      <td>539</td>\n",
              "      <td>\u00bc cup butter ; \u00bc cup soy sauce ; 1 bunch green...</td>\n",
              "      <td>Melt butter in a skillet, and mix in the soy s...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['Melt butter in a skillet over medium heat; s...</td>\n",
              "      <td>https://www.allrecipes.com/thmb/mYkvln7o9pb35l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Quick and Easy Pizza Crust</td>\n",
              "      <td>https://www.allrecipes.com/recipe/20171/quick-...</td>\n",
              "      <td>bread</td>\n",
              "      <td>CHEF RIDER</td>\n",
              "      <td>This is a great recipe when you don't want to ...</td>\n",
              "      <td>4.70</td>\n",
              "      <td>3741</td>\n",
              "      <td>2794</td>\n",
              "      <td>1 (.25 ounce) package active dry yeast ; 1 tea...</td>\n",
              "      <td>Preheat oven to 450 degrees F (230 degrees C)....</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['Preheat oven to 450 degrees F (230 degrees C...</td>\n",
              "      <td>https://www.allrecipes.com/thmb/V3Llo-ottudIs_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>Spicy Grilled Shrimp</td>\n",
              "      <td>https://www.allrecipes.com/recipe/12775/spicy-...</td>\n",
              "      <td>seafood</td>\n",
              "      <td>SUBEAST</td>\n",
              "      <td>So fast and easy to prepare, these shrimp are ...</td>\n",
              "      <td>4.60</td>\n",
              "      <td>1121</td>\n",
              "      <td>836</td>\n",
              "      <td>1 large clove garlic ; 1 teaspoon coarse salt ...</td>\n",
              "      <td>Preheat grill for medium heat. In a small bowl...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['Preheat a grill for medium heat.', 'Crush ga...</td>\n",
              "      <td>https://www.allrecipes.com/thmb/bH-TT4Cifwew96...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>Grilled Chicken Marinade</td>\n",
              "      <td>https://www.allrecipes.com/recipe/241890/grill...</td>\n",
              "      <td>meat-and-poultry</td>\n",
              "      <td>Jennifer</td>\n",
              "      <td>Simply the best chicken marinade for any occas...</td>\n",
              "      <td>4.77</td>\n",
              "      <td>236</td>\n",
              "      <td>167</td>\n",
              "      <td>\u00bc cup red wine vinegar ; \u00bc cup reduced-sodium ...</td>\n",
              "      <td>Whisk vinegar, soy sauce, olive oil, parsley, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['Whisk vinegar, soy sauce, olive oil, parsley...</td>\n",
              "      <td>https://www.allrecipes.com/thmb/b8GwNUs4HIz9-X...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>Wool Roll Bread</td>\n",
              "      <td>https://www.allrecipes.com/recipe/284058/wool-...</td>\n",
              "      <td>uncategorized</td>\n",
              "      <td>Chef John</td>\n",
              "      <td>I've spun quite a few yarns but one thing I've...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>\u00bd cup water ; \u00bc cup all-purpose flour ; \u00bd cup ...</td>\n",
              "      <td>To make \"water roux,\" whisk together water and...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['To make \"water roux,\" whisk together water a...</td>\n",
              "      <td>https://www.allrecipes.com/thmb/BL8waIMOSis0wl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>Easy Pizza Sauce I</td>\n",
              "      <td>https://www.allrecipes.com/recipe/11771/easy-p...</td>\n",
              "      <td>side-dish</td>\n",
              "      <td>Frank Sweterlitsch</td>\n",
              "      <td>A simple pizza sauce used by many pizzerias. T...</td>\n",
              "      <td>4.34</td>\n",
              "      <td>570</td>\n",
              "      <td>430</td>\n",
              "      <td>1 (6 ounce) can tomato paste ; 1\u2009\u00bd cups water ...</td>\n",
              "      <td>Mix together the tomato paste, water, and oliv...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['Mix together water, tomato paste, and olive ...</td>\n",
              "      <td>https://www.allrecipes.com/thmb/aYLD9MOpFl7p7A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>Portuguese Custard Tarts (Pasteis de Nata)</td>\n",
              "      <td>https://www.allrecipes.com/recipe/269064/portu...</td>\n",
              "      <td>world-cuisine</td>\n",
              "      <td>Chef John</td>\n",
              "      <td>This slightly streamlined recipe for the world...</td>\n",
              "      <td>4.85</td>\n",
              "      <td>108</td>\n",
              "      <td>69</td>\n",
              "      <td>1 cup all-purpose flour ; \u00bc teaspoon kosher sa...</td>\n",
              "      <td>Combine flour, salt, and cold water in a bowl....</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[\"Combine flour, salt, and cold water in a bow...</td>\n",
              "      <td>https://www.allrecipes.com/thmb/aYhkh4DtydnJYQ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows \u00d7 49 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b785fe45-ac1d-4d15-8b24-84d10a77e99d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b785fe45-ac1d-4d15-8b24-84d10a77e99d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b785fe45-ac1d-4d15-8b24-84d10a77e99d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                          title  \\\n",
              "0                    Simple Macaroni and Cheese   \n",
              "1                      Gourmet Mushroom Risotto   \n",
              "2                                Dessert Crepes   \n",
              "3                                   Pork Steaks   \n",
              "4                    Quick and Easy Pizza Crust   \n",
              "..                                          ...   \n",
              "495                        Spicy Grilled Shrimp   \n",
              "496                    Grilled Chicken Marinade   \n",
              "497                             Wool Roll Bread   \n",
              "498                          Easy Pizza Sauce I   \n",
              "499  Portuguese Custard Tarts (Pasteis de Nata)   \n",
              "\n",
              "                                                   url              category  \\\n",
              "0    https://www.allrecipes.com/recipe/238691/simpl...             main-dish   \n",
              "1    https://www.allrecipes.com/recipe/85389/gourme...             main-dish   \n",
              "2    https://www.allrecipes.com/recipe/19037/desser...  breakfast-and-brunch   \n",
              "3    https://www.allrecipes.com/recipe/70463/pork-s...      meat-and-poultry   \n",
              "4    https://www.allrecipes.com/recipe/20171/quick-...                 bread   \n",
              "..                                                 ...                   ...   \n",
              "495  https://www.allrecipes.com/recipe/12775/spicy-...               seafood   \n",
              "496  https://www.allrecipes.com/recipe/241890/grill...      meat-and-poultry   \n",
              "497  https://www.allrecipes.com/recipe/284058/wool-...         uncategorized   \n",
              "498  https://www.allrecipes.com/recipe/11771/easy-p...             side-dish   \n",
              "499  https://www.allrecipes.com/recipe/269064/portu...         world-cuisine   \n",
              "\n",
              "                    author                                        description  \\\n",
              "0              g0dluvsugly  A very quick and easy fix to a tasty side-dish...   \n",
              "1    Myleen Sagrado Sj\u00f6din  Authentic Italian-style risotto cooked the slo...   \n",
              "2                    ANN57  Essential crepe recipe.  Sprinkle warm crepes ...   \n",
              "3             BABYLOVE1222  My mom came up with this recipe when I was a c...   \n",
              "4               CHEF RIDER  This is a great recipe when you don't want to ...   \n",
              "..                     ...                                                ...   \n",
              "495                SUBEAST  So fast and easy to prepare, these shrimp are ...   \n",
              "496               Jennifer  Simply the best chicken marinade for any occas...   \n",
              "497              Chef John  I've spun quite a few yarns but one thing I've...   \n",
              "498     Frank Sweterlitsch  A simple pizza sauce used by many pizzerias. T...   \n",
              "499              Chef John  This slightly streamlined recipe for the world...   \n",
              "\n",
              "     rating  rating_count  review_count  \\\n",
              "0      4.42           834           575   \n",
              "1      4.80          3388          2245   \n",
              "2      4.80          1156           794   \n",
              "3      4.57           689           539   \n",
              "4      4.70          3741          2794   \n",
              "..      ...           ...           ...   \n",
              "495    4.60          1121           836   \n",
              "496    4.77           236           167   \n",
              "497    0.00             0             0   \n",
              "498    4.34           570           430   \n",
              "499    4.85           108            69   \n",
              "\n",
              "                                           ingredients  \\\n",
              "0    1 (8 ounce) box elbow macaroni ; \u00bc cup butter ...   \n",
              "1    6 cups chicken broth, divided ; 3 tablespoons ...   \n",
              "2    4  eggs, lightly beaten ; 1\u2009\u2153 cups milk ; 2 ta...   \n",
              "3    \u00bc cup butter ; \u00bc cup soy sauce ; 1 bunch green...   \n",
              "4    1 (.25 ounce) package active dry yeast ; 1 tea...   \n",
              "..                                                 ...   \n",
              "495  1 large clove garlic ; 1 teaspoon coarse salt ...   \n",
              "496  \u00bc cup red wine vinegar ; \u00bc cup reduced-sodium ...   \n",
              "497  \u00bd cup water ; \u00bc cup all-purpose flour ; \u00bd cup ...   \n",
              "498  1 (6 ounce) can tomato paste ; 1\u2009\u00bd cups water ...   \n",
              "499  1 cup all-purpose flour ; \u00bc teaspoon kosher sa...   \n",
              "\n",
              "                                            directions  ... vitamin_k_mcg  \\\n",
              "0    Bring a large pot of lightly salted water to a...  ...           NaN   \n",
              "1    In a saucepan, warm the broth over low heat. W...  ...           NaN   \n",
              "2    In large bowl, whisk together eggs, milk, melt...  ...           NaN   \n",
              "3    Melt butter in a skillet, and mix in the soy s...  ...           NaN   \n",
              "4    Preheat oven to 450 degrees F (230 degrees C)....  ...           NaN   \n",
              "..                                                 ...  ...           ...   \n",
              "495  Preheat grill for medium heat. In a small bowl...  ...           NaN   \n",
              "496  Whisk vinegar, soy sauce, olive oil, parsley, ...  ...           NaN   \n",
              "497  To make \"water roux,\" whisk together water and...  ...           NaN   \n",
              "498  Mix together the tomato paste, water, and oliv...  ...           NaN   \n",
              "499  Combine flour, salt, and cold water in a bowl....  ...           NaN   \n",
              "\n",
              "    biotin_mcg vitamin_b12_mcg  mono_fat_g poly_fat_g  trans_fatty_acid_g  \\\n",
              "0          NaN             NaN         NaN        NaN                 NaN   \n",
              "1          NaN             NaN         NaN        NaN                 NaN   \n",
              "2          NaN             NaN         NaN        NaN                 NaN   \n",
              "3          NaN             NaN         NaN        NaN                 NaN   \n",
              "4          NaN             NaN         NaN        NaN                 NaN   \n",
              "..         ...             ...         ...        ...                 ...   \n",
              "495        NaN             NaN         NaN        NaN                 NaN   \n",
              "496        NaN             NaN         NaN        NaN                 NaN   \n",
              "497        NaN             NaN         NaN        NaN                 NaN   \n",
              "498        NaN             NaN         NaN        NaN                 NaN   \n",
              "499        NaN             NaN         NaN        NaN                 NaN   \n",
              "\n",
              "     omega_3_fatty_acid_g  omega_6_fatty_acid_g  \\\n",
              "0                     NaN                   NaN   \n",
              "1                     NaN                   NaN   \n",
              "2                     NaN                   NaN   \n",
              "3                     NaN                   NaN   \n",
              "4                     NaN                   NaN   \n",
              "..                    ...                   ...   \n",
              "495                   NaN                   NaN   \n",
              "496                   NaN                   NaN   \n",
              "497                   NaN                   NaN   \n",
              "498                   NaN                   NaN   \n",
              "499                   NaN                   NaN   \n",
              "\n",
              "                                     instructions_list  \\\n",
              "0    ['Bring a large pot of lightly salted water to...   \n",
              "1    ['Warm broth in a saucepan over low heat.', 'M...   \n",
              "2    ['Whisk together eggs, milk, flour, melted but...   \n",
              "3    ['Melt butter in a skillet over medium heat; s...   \n",
              "4    ['Preheat oven to 450 degrees F (230 degrees C...   \n",
              "..                                                 ...   \n",
              "495  ['Preheat a grill for medium heat.', 'Crush ga...   \n",
              "496  ['Whisk vinegar, soy sauce, olive oil, parsley...   \n",
              "497  ['To make \"water roux,\" whisk together water a...   \n",
              "498  ['Mix together water, tomato paste, and olive ...   \n",
              "499  [\"Combine flour, salt, and cold water in a bow...   \n",
              "\n",
              "                                                 image  \n",
              "0    https://www.allrecipes.com/thmb/GZrTl8DBwmRuor...  \n",
              "1    https://www.allrecipes.com/thmb/xCk4IEjfAYBikO...  \n",
              "2    https://www.allrecipes.com/thmb/VwULr05JFDluPI...  \n",
              "3    https://www.allrecipes.com/thmb/mYkvln7o9pb35l...  \n",
              "4    https://www.allrecipes.com/thmb/V3Llo-ottudIs_...  \n",
              "..                                                 ...  \n",
              "495  https://www.allrecipes.com/thmb/bH-TT4Cifwew96...  \n",
              "496  https://www.allrecipes.com/thmb/b8GwNUs4HIz9-X...  \n",
              "497  https://www.allrecipes.com/thmb/BL8waIMOSis0wl...  \n",
              "498  https://www.allrecipes.com/thmb/aYLD9MOpFl7p7A...  \n",
              "499  https://www.allrecipes.com/thmb/aYhkh4DtydnJYQ...  \n",
              "\n",
              "[500 rows x 49 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"hf://datasets/Shengtao/recipe/recipe.csv\")\n",
        "df = df.head(500)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60rK4KTp__rR"
      },
      "source": [
        "Now I am going to combine the `title`, `directions`, and `ingredients` columns into one entry per row. This will be what we embed for my model to reference using RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nh0jycjADWd",
        "outputId": "074aca54-3e98-40d2-ddc7-86fcd7b7c904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Made recipe list\n"
          ]
        }
      ],
      "source": [
        "text = []\n",
        "for i, row in df.iterrows():\n",
        "    text.append(row['title'] + \": \" + row['directions'] + \" Ingredients list: \" + row['ingredients'])\n",
        "\n",
        "print(\"Made recipe list\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v09u15OlAG1z"
      },
      "source": [
        "---\n",
        "### Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypt_PSMxAH_x"
      },
      "source": [
        "Now I am going to setup my embedding model. We will be using the [SentenceTransformers](https://sbert.net/) library for this example but there are lots of other libraries/frameworks out there for doing RAG.\n",
        "\n",
        "We will be using the `all-mpnet-base-v2` model which offers a nice balance of size and performance. There are many models available through SentenceTransformers. Some are more accurate but larger and slower to run while others are more lightweight but may suffer in performance. However if you would like to learn more or try out a different model see the [documentation linked here](https://sbert.net/docs/sentence_transformer/pretrained_models.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "c677109f78d14b338540c986c2045807",
            "cb074d613835461288f3a79e6a3561fc",
            "a30fcdf4d6c44b4cb48e9c0a63983386",
            "a60edeb95de447958486c7604219a5d8",
            "c47484e56fc1463da453a4fbd461f07b",
            "d334ac031a854ffeb4b39bcca841a95f",
            "bbb306b048454718a6ac1ff50b3dce9a",
            "4a943b904490449690616445da5dce66",
            "b5ec07c7b4c84438afafd7ef9a49ad21",
            "9926a521803f4cff8b6d6dbda78aa322",
            "a775499f5b224fad8a543609c08c653d",
            "2520c7d4408e453b9c10384552137b1a",
            "0c5c6b04d3b047958a01484cb22f0511",
            "72054ea3cc2345959f4bd533a2fefc9f",
            "c54e3cb602f044e782bc6b278a822f56",
            "52f6baeb524841e2b0a0c769ed42bc21",
            "1427707ac83a4b81acf5b4c45e48ea8a",
            "bcb7f58253dc461a9b8ef815493fa594",
            "370709d1199149ef8a569868c5c9d809",
            "f9a0bd47bfe64fb4a2c0f969e98b3a72",
            "85f662faa3404365947d8ffab518f44e",
            "f0c497d5edd1417b97188ae8801eef52",
            "a5973ce9095d4c6eb7c2000ef04a9e13",
            "319b669176994d849b2828a37bbb15f0",
            "58a04357032944dab8c6ff80ca497557",
            "d87a7d27802e4372b184da1f348a10c9",
            "554a2f34beec4792a423df11dea98b00",
            "dd76cb556b81427da2f7b91fe12e035a",
            "056a1d94a5674089a3030105eaac0756",
            "c6c8aaff50ab4b948a21f9e9d89ddfd0",
            "588fa9e260eb4db1bf0f1066c2a0d93b",
            "ca90f4988a7a4e2bbd84aee343b2688f",
            "944acfc7f27846be8e85a98b942b27df",
            "77ed7703946044b2b923236edc07bfca",
            "615a3dcb26304897a7d0ce0acd08709e",
            "3744ba4e36184e2aa2f6edd15ebb8669",
            "577c6f7a5db84df58a118c1e774dd15c",
            "27c20aab3d244c5394e64eef3cae92f2",
            "279b0cacd75f4de48c3ae04e4509c1f3",
            "cc2d2c7269e94333a48e9451a681cce5",
            "d20aa01233ab43e0b819d3b457a8c011",
            "cfaeea8b46ad497cbca57186998dd624",
            "4e97200cea344d4bb0e0ae499f53ec24",
            "3f9bce7f77ac4c3e865d4aeb7832e8fb",
            "d4d467fe91444ee3b414a53f3445e349",
            "260d3b82f014485e984e28b3063b4c58",
            "7efbe396790c4963800891ccf6bab9f3",
            "011731a0a0e345d9af8ddbc74eaea812",
            "8747ce8fa49746d3b7ea92c3ef658d98",
            "86ddd7f66fdd4d5ea6ee5d3b2fb75e5b",
            "b279fc7349f14274ab9e4ebf16f5465f",
            "bf18c85436b840b4a46db2ba3330feaa",
            "1c346ebf114d4425b7d96fdcf31ebeab",
            "44371737a243459fbf707cebf94d877f",
            "7432c3724b964544bf27048ab6074f6d",
            "88cb620b644349d399169872aa9d1f18",
            "664bb1bdfbee46b298e44120e9545fa3",
            "8894b40e33b2483ea2aa97d43657cf5d",
            "bec34649622a42e78c4734f456489453",
            "0408ea2fe4b34ec3b21dd9a0b4fd1d69",
            "6a0d864b10d64b599d1313cd1821a12c",
            "78384190bdf8415d827f0e87ae7b4f31",
            "3a78280b29c649cbbcb81e48e4481542",
            "c89cbde2b30f4d99b306ad69ebd5dd07",
            "66e568095982473b83be372c87e91ca2",
            "f167a47ce7a740009c51a7a7be1b9463",
            "75cbe2067ee2482d8681bd3cafe90f73",
            "fd3151d286844474ad44f436bb11d462",
            "e5dbb0b117a547f3b7606e414d8dc789",
            "4bf04ebdb6ab409eae20ce37d5fe7603",
            "ebc3c8e5cc0548fcb4f0854620eb6749",
            "92eec920361949a9a0aa23d61fd7b0a2",
            "8fa0e8ab0ff54c40802be2ba64d4b430",
            "91b5259b08c34999bfcc60f1df5bfd6f",
            "5d8cee8ab95d459da9260fb0c5b78230",
            "4296a6184eaf40889478d1b5fd750fa9",
            "4f6285c8845b4b668e389e96ee04b337",
            "9752930fc836411f96d84c4f6bbdc9b1",
            "81b015787e4940359956de039385337b",
            "3d3e1aca7a324627b770762f9154da80",
            "618aa40503534ff39ee7c35e0799e96e",
            "7d8b82504db34bc1bcdc9c6beaa9cec3",
            "b155638c8f2b4999aa82f5faea973d05",
            "f40264e9e1684fb2bb83b6b2ee122c0e",
            "89c687fa3c854648bd0cfd9847e2d45b",
            "0652d8da404a4e159eb5dffd2cd9063d",
            "55702be0279045c184c6f375e6382068",
            "2faf6abbdc0c41298c9f35c52683a4cb",
            "d2133eccef9b46c4af4aad1c63519067",
            "69cb8f9f720245ec8d95e4f9425d157c",
            "54a1f2b2c3a744a7a0aafa647f1813d8",
            "3979b4cc50cf407bb82676c328a690b8",
            "81d74c704e17490f84677ba4e2c2630f",
            "47522d4247c241c28937131e97ca13de",
            "65bd9e29747545c9b4f16f10288cd354",
            "3dd992ba19244612a72da211195d0e5f",
            "4025d4ed32914e04af0e925a5978f11a",
            "d64a191a1bcd40d5b2a5e10ab8cc6ba1",
            "25890ec423ee421db20037a60ce4f402",
            "18cda9544d564649b254a0ca6b5b2418",
            "5340d82292c74dea8534dcab258e4273",
            "d02987cb78914a178166dafdafcac142",
            "351bb7f1bf834abca8d5e31e6dcc7f0b",
            "6830c5c9761e4af1a8fd62f04b4c2f7e",
            "9614295b513342e68675b11d60356c90",
            "e0f42f2d24c94ff98cf39d33f046c1e6",
            "6828abec589d4dd9bb7c0410ba21d0be",
            "8a60b30a7d484510994edcdfccc525fc",
            "6b2a07e0d4954e30a44a3bcf770837ef",
            "4662b83152924b05bc71b11f794403e2",
            "920a293097a64fe1965e68a8c1fda8fd",
            "536f2179d5e648c39c524cf90b5b0c97",
            "b1617873684e4fc38a7ad2d75d44f4da",
            "b5735655aa3543239d6b60e86997178a",
            "13d7e9d7b3f44e45bdb868412cc1cd4e",
            "4742222766fe4e15b2c8a94e5f41d5d9",
            "fd6d9f7e63e54c7c8f98979db14b8df0",
            "46dc6e36cec949abb318b03c86039d23",
            "7adb13381bba457b8394ee4042e31a0e",
            "e6b28582c7b44717bf7a471fe69b3827",
            "3e085ca272f446a1b85aba2cd37eeed6"
          ]
        },
        "id": "je8E946DALiw",
        "outputId": "e70b0af8-60b1-4357-ce5d-ce987e548dd8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8864b6175b784dbca04270b2c7c2fc92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd9133e0e9bc411583fa12a2df29a38a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f31c8eacb07b45ebbbed29d1acf93b32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05cf03afb67f46adbae1f5f304eeff21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e944d37f9b0444c19c3f46dd60d0fb58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cec0bc27c434b4cb8b47f3a102a9991",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1096d759c38c47369775858a6eaa7934",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MPNetModel LOAD REPORT from: sentence-transformers/all-mpnet-base-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae8b50ecb4d847c9ab420e498cddb8fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5557cd3119244d8e8bff827708b45783",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7a64d755744460da78a075ad8f39baa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31c64e3c5b6b47389daa3f5a98a90768",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d863c4670e1434eab1625c31d1e3459",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "st_model = SentenceTransformer(\"all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri45Ef09AOLS"
      },
      "source": [
        "In the cell below we take each entry in my list of recipes and embed it using the `all-mpnet-base-v2` model. `.encode` creates my embeddings which we append to a list called `embeddings`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFCFznekARPu",
        "outputId": "6c77488f-f1f6-4644-83d4-96c5f1740ca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Made recipe embeddings\n"
          ]
        }
      ],
      "source": [
        "embeddings = []\n",
        "\n",
        "for line in text:\n",
        "    embedding = st_model.encode(line, convert_to_tensor=True)\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "print(\"Made recipe embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GndGbDYxALDa"
      },
      "source": [
        "---\n",
        "### Inference Using RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e8aff2f"
      },
      "source": [
        "**Note on Storing Embeddings:**\n",
        "\n",
        "In this demo, embeddings are stored in a Python list embeddings = [] in memory for simplicity. However, for real-world RAG systems with large datasets, it's highly recommended to use a **vector database** (or vector store). These databases are optimized for efficiently storing and searching large collections of high-dimensional embeddings, enabling fast and scalable retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H7Evp8_Aa0V"
      },
      "source": [
        "Now I am ready to see the results of using my embeddings in my prompts for `Falcon3-7B-Base`. For each prompt in my list of prompts we run through the loop below. We first encode the prompt and then use `semantic_search` from SentenceTransformers to find the most similar embedded recipes. With the `top_k=2` argument we only get the top two results. Then we reference which index these results correspond to and fetch the appropriate elements from my `text` recipe list. These results are included in the prompt and the prompt is passed to my LLM just like before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhWB27nwAftB",
        "outputId": "70a3eb1f-7492-48e2-941c-03d5f422e3dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Two-Ingredient Pizza Dough: Mix flour and Greek yogurt together in a bowl; transfer to a work surface floured with self-rising flour. Knead dough, adding more flour as needed to keep dough from being too sticky, for 8 to 10 minutes. Spray a 12-inch pizza pan with cooking spray and spread dough to edges of pan. Ingredients list: 1\u2009\u00bd cups self-rising flour, plus more for kneading ; 1 cup plain Greek yogurt ;   cooking spray\n",
            "No-Yeast Pizza Crust: Mix flour, baking powder, and salt together in a bowl; stir in milk and olive oil until a soft dough forms. Turn dough onto a lightly floured surface and knead 10 times. Shape dough into a ball. Cover dough with an inverted bowl and let sit for 10 minutes. Roll dough into a 12-inch circle on a baking sheet. Ingredients list: 1\u2009\u2153 cups all-purpose flour ; 1 teaspoon baking powder ; \u00bd teaspoon salt ; \u00bd cup fat-free milk ; 2 tablespoons olive oil\n",
            "\n",
            "Question: What two ingredients do I need for two-ingredient pizza dough?\n",
            "Answer: 1\u2009\u00bd cups self-rising flour and 1 cup plain Greek yogurt.\n",
            "\n",
            "Question: What is the main\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Quick Tartar Sauce: Stir the mayonnaise, relish, mustard, and lemon juice together in a bowl. Ingredients list: 1 cup mayonnaise ; 2 teaspoons sweet pickle relish ; 1 teaspoon prepared yellow mustard ; 1 teaspoon lemon juice\n",
            "Tartar Sauce I: In a small bowl, mix together mayonnaise, sweet pickle relish, and minced onion. Stir in lemon juice.  Season to taste with salt and pepper. Refrigerate for at least 1 hour before serving. Ingredients list: 1 cup mayonnaise ; 1 tablespoon sweet pickle relish ; 1 tablespoon minced onion ; 2 tablespoons lemon juice  (Optional);   salt and pepper to taste\n",
            "\n",
            "Question: What do I need for a quick tartar sauce?\n",
            "Answer: 1 cup mayonnaise, 2 teaspoons sweet pickle relish, 1 teaspoon prepared yellow mustard, and 1 teaspoon lemon juice.\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "To Die For Blueberry Muffins: Preheat oven to 400 degrees F (200 degrees C). Grease muffin cups or line with muffin liners. Combine 1 1/2 cups flour, 3/4 cup sugar, salt and baking powder. Place vegetable oil into a 1 cup measuring cup; add the egg and add enough milk to reach the 1-cup mark. Mix this with flour mixture. Fold in blueberries. Fill muffin cups right to the top, and sprinkle with crumb topping mixture. To Make Crumb Topping: Mix together 1/2 cup sugar, 1/3 cup flour, 1/4 cup butter,  and 1 1/2 teaspoons cinnamon. Mix with fork, and sprinkle over muffins before baking. Bake for 20 to 25 minutes in the preheated oven, or until done. Ingredients list: 1\u2009\u00bd cups all-purpose flour ; \u00be cup white sugar ; \u00bd teaspoon salt ; 2 teaspoons baking powder ; \u2153 cup vegetable oil ; 1  egg ; \u2153 cup milk, or more as needed ; 1 cup fresh blueberries ; \u00bd cup white sugar ; \u2153 cup all-purpose flour ; \u00bc cup butter, cubed ; 1\u2009\u00bd teaspoons ground cinnamon\n",
            "Best Ever Muffins: Preheat oven to 400 degrees F (205 degrees C). Stir together the flour, baking powder, salt and sugar in a large bowl.  Make a well in the center. In a small bowl or 2 cup measuring cup, beat egg with a fork. Stir in milk and oil. Pour all at once into the well in the flour mixture. Mix quickly and lightly with a fork until moistened, but do not beat. The batter will be lumpy. Pour the batter into paper lined muffin pan cups. Variations: Blueberry Muffins: Add 1 cup fresh blueberries. Raisin Muffins: Add 1 cup finely chopped raisins. Date Muffins: Add 1 cup finely chopped dates. Cheese Muffins: Fold in 1 cup grated sharp yellow cheese. Bacon Muffins: Fold 1/4 cup crisp cooked bacon, broken into bits. Bake for 25 minutes, or until golden. Ingredients list: 2 cups all-purpose flour ; 3 teaspoons baking powder ; \u00bd teaspoon salt ; \u00be cup white sugar ; 1  egg ; 1 cup milk ; \u00bc cup vegetable oil\n",
            "\n",
            "Question: What should I preheat the oven to to make blueberry muffins?\n",
            "Answer: 400 degrees F (200 degrees C)\n",
            "\n",
            "Question: What should I do after I mix the\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "All done\n"
          ]
        }
      ],
      "source": [
        "for prompt in prompts:\n",
        "    prompt = prompt_task + prompt + prompt_end\n",
        "\n",
        "    prompt_embed = st_model.encode(prompt, convert_to_tensor=True)\n",
        "    hits = semantic_search(prompt_embed, embeddings, top_k=2)\n",
        "\n",
        "    result_line1 = hits[0][0]['corpus_id']\n",
        "    result_line2 = hits[0][1]['corpus_id']\n",
        "\n",
        "    prompt = text[result_line1] + \"\\n\" + text[result_line2] + \"\\n\\n\" + prompt\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    generate_ids = falcon_model.generate(input_ids=inputs.input_ids.to('cuda'), attention_mask=inputs.attention_mask.to('cuda'), max_new_tokens=25, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(tokenizer.decode(generate_ids[0]))\n",
        "    print(\"\\n------------------------------------------------------------\")\n",
        "\n",
        "print(\"\\n\\nAll done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu-OzYiHVxRy"
      },
      "source": [
        "In these results you can see the dataset entries it pulled appended to the beginning of each prompt. The LLM then answers the questions using these bits of information from my dataset, giving different answers to the previous inference example without RAG. This demonstrates how I can use RAG to add information we would like my model to reference when answering my questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whV-cBV5AUHM"
      },
      "source": [
        "---\n",
        "\n",
        "###Helpful Resources for Further Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXEWzETIeZDp"
      },
      "source": [
        "As mentioned in the lecture portion of this project, we did not have time to cover many important aspects of how LLMs work. Below I have links to resources I found useful in understanding how Transformers work as well as other concepts mentioned in this session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7NwLwpyAa5K"
      },
      "source": [
        "Papers:\n",
        "*   8 Things to Known about LLMs paper: https://arxiv.org/abs/2304.00612\n",
        "*   Attention is All You Need (original transformers paper): https://arxiv.org/abs/1706.03762\n",
        "*   LoRA: Low-Rank Adaptation of Large Language Models: https://arxiv.org/abs/2106.09685\n",
        "*   QLoRA: Efficient Finetuning of Quantized LLMs: https://arxiv.org/abs/2305.14314\n",
        "\n",
        "Blogs:\n",
        "*   Blog explaining building a Transformer model from scratch: https://peterbloem.nl/blog/transformers\n",
        "*   Blog on Bits and Bytes: https://huggingface.co/blog/hf-bitsandbytes-integration\n",
        "*   Hugging Face's Blog on Falcon: https://huggingface.co/blog/falcon\n",
        "*   Using LoRA for finetuning: https://dataman-ai.medium.com/fine-tune-a-gpt-lora-e9b72ad4ad3\n",
        "*   LoRA Parameter Overview: https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms\n",
        "\n",
        "\n",
        "YouTube videos:\n",
        "*   University of Waterloo lecture on Attention and Transformers: https://www.youtube.com/watch?v=OyFJWRnt_AY\n",
        "*   Attention is All You Need paper explanation: https://www.youtube.com/watch?v=w76Dpp7b3B4\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}